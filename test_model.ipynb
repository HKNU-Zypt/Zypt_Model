{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9bc75f-ac8a-40a1-a6ad-8bbafc2f595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746154325.602326   55690 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1746154325.610660   56523 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746154325.612940   56528 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "# Mediapipe 얼굴 랜드마크 초기화\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "# EXIF 회전 보정 함수\n",
    "def load_image_with_exif_orientation(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    \n",
    "    try:\n",
    "        exif = image._getexif()\n",
    "        if exif is not None:\n",
    "            for orientation in ExifTags.TAGS.keys():\n",
    "                if ExifTags.TAGS[orientation] == 'Orientation':\n",
    "                    break\n",
    "            if orientation in exif:\n",
    "                orientation_value = exif[orientation]\n",
    "                if orientation_value == 3:\n",
    "                    image = image.rotate(180, expand=True)\n",
    "                elif orientation_value == 6:\n",
    "                    image = image.rotate(270, expand=True)\n",
    "                elif orientation_value == 8:\n",
    "                    image = image.rotate(90, expand=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return np.array(image)\n",
    "\n",
    "# 이미지 리사이즈 함수\n",
    "def resize_image(image, max_width=640):\n",
    "    h, w = image.shape[:2]\n",
    "    if w > max_width:\n",
    "        scale = max_width / w\n",
    "        image = cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "    return image\n",
    "\n",
    "# 얼굴 랜드마크 추출 함수\n",
    "def extract_face_landmarks(image_path, normalize=True):\n",
    "    image = load_image_with_exif_orientation(image_path)\n",
    "    image = resize_image(image)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    results = face_mesh.process(image_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks = results.multi_face_landmarks[0]\n",
    "        coords = np.array([[lm.x, lm.y] for lm in landmarks.landmark])\n",
    "        \n",
    "        if normalize:\n",
    "            coords -= np.mean(coords, axis=0)\n",
    "        \n",
    "        return coords\n",
    "    \n",
    "    return None  # 얼굴을 찾지 못한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4465f0b4-4566-412f-8f8a-2eb64809dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted emotion: Focused\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "model = load_model(\"concentration_model.h5\")  # 저장된 모델 경로로 변경\n",
    "\n",
    "# 감정 클래스 라벨 정의\n",
    "emotion_labels = {0: \"Focused\", 1: \"Not Focused\", 2: \"Drowsy\"}\n",
    "\n",
    "# 이미지 테스트 함수\n",
    "def test_image(image_path):\n",
    "    # 얼굴 랜드마크 추출\n",
    "    landmarks = extract_face_landmarks(image_path)\n",
    "    \n",
    "    if landmarks is not None:\n",
    "        # 모델에 맞게 데이터 차원 변경\n",
    "        landmarks = np.expand_dims(landmarks, axis=0)  # (1, 468, 2)로 변경\n",
    "        \n",
    "        # 예측\n",
    "        prediction = model.predict(landmarks)\n",
    "        \n",
    "        # 예측된 라벨을 감정 라벨로 변환\n",
    "        predicted_class = np.argmax(prediction, axis=1)\n",
    "        predicted_emotion = emotion_labels[predicted_class[0]]  # 감정 클래스 라벨을 변환\n",
    "\n",
    "        print(f\"Predicted emotion: {predicted_emotion}\")  # 예측된 감정 출력\n",
    "\n",
    "    else:\n",
    "        print(\"No face detected in the image.\")\n",
    "\n",
    "# 테스트할 이미지 경로\n",
    "image_path = \"/Users/kimjohyeon/Desktop/Capstone/Sample2.jpg\"  # 테스트할 이미지 경로로 변경\n",
    "\n",
    "# 테스트 이미지로 예측 수행\n",
    "test_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259986d-841a-494a-9d70-01f8c3a51312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
