{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04fe1110",
   "metadata": {},
   "source": [
    "## TFLite 모델을 사용한 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9bc75f-ac8a-40a1-a6ad-8bbafc2f595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746154325.602326   55690 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1746154325.610660   56523 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746154325.612940   56528 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "# Mediapipe 얼굴 랜드마크 초기화\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "# EXIF 회전 보정 함수\n",
    "def load_image_with_exif_orientation(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    \n",
    "    try:\n",
    "        exif = image._getexif()\n",
    "        if exif is not None:\n",
    "            for orientation in ExifTags.TAGS.keys():\n",
    "                if ExifTags.TAGS[orientation] == 'Orientation':\n",
    "                    break\n",
    "            if orientation in exif:\n",
    "                orientation_value = exif[orientation]\n",
    "                if orientation_value == 3:\n",
    "                    image = image.rotate(180, expand=True)\n",
    "                elif orientation_value == 6:\n",
    "                    image = image.rotate(270, expand=True)\n",
    "                elif orientation_value == 8:\n",
    "                    image = image.rotate(90, expand=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return np.array(image)\n",
    "\n",
    "# 이미지 리사이즈 함수\n",
    "def resize_image(image, max_width=640):\n",
    "    h, w = image.shape[:2]\n",
    "    if w > max_width:\n",
    "        scale = max_width / w\n",
    "        image = cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "    return image\n",
    "\n",
    "# 얼굴 랜드마크 추출 함수\n",
    "def extract_face_landmarks(image_path, normalize=True):\n",
    "    image = load_image_with_exif_orientation(image_path)\n",
    "    image = resize_image(image)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    results = face_mesh.process(image_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks = results.multi_face_landmarks[0]\n",
    "        coords = np.array([[lm.x, lm.y] for lm in landmarks.landmark])\n",
    "        \n",
    "        if normalize:\n",
    "            coords -= np.mean(coords, axis=0)\n",
    "        \n",
    "        return coords\n",
    "    \n",
    "    return None  # 얼굴을 찾지 못한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4465f0b4-4566-412f-8f8a-2eb64809dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted emotion: Focused\n"
     ]
    }
   ],
   "source": [
    "# TFLite 모델 로드\n",
    "interpreter = tf.lite.Interpreter(model_path=\"concentration_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# 감정 클래스 라벨 정의\n",
    "emotion_labels = {0: \"Focused\", 1: \"Not Focused\", 2: \"Drowsy\"}\n",
    "\n",
    "# 이미지 테스트 함수\n",
    "def test_image(image_path):\n",
    "    # 얼굴 랜드마크 추출\n",
    "    landmarks = extract_face_landmarks(image_path)\n",
    "    \n",
    "    if landmarks is not None:\n",
    "        # 모델에 맞게 데이터 차원 변경\n",
    "        landmarks = np.expand_dims(landmarks, axis=0).astype(np.float32)  # (1, 468, 2)로 변경\n",
    "        \n",
    "        # 입력 텐서에 데이터 넣기\n",
    "        input_details = interpreter.get_input_details()\n",
    "        input_index = input_details[0]['index']\n",
    "        interpreter.set_tensor(input_index, landmarks)\n",
    "\n",
    "        # 추론 실행\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # 출력 텐서에서 예측값 가져오기\n",
    "        output_details = interpreter.get_output_details()\n",
    "        output_index = output_details[0]['index']\n",
    "        prediction = interpreter.get_tensor(output_index)\n",
    "\n",
    "        # 예측된 라벨을 감정 라벨로 변환\n",
    "        predicted_class = np.argmax(prediction, axis=1)\n",
    "        predicted_emotion = emotion_labels[predicted_class[0]]  # 감정 클래스 라벨을 변환\n",
    "\n",
    "        print(f\"Predicted emotion: {predicted_emotion}\")  # 예측된 감정 출력\n",
    "\n",
    "    else:\n",
    "        print(\"No face detected in the image.\")\n",
    "\n",
    "# 테스트할 이미지 경로\n",
    "image_path = \"/Users/kimjohyeon/Desktop/Capstone/Sample2.jpg\"  # 테스트할 이미지 경로로 변경\n",
    "\n",
    "# 테스트 이미지로 예측 수행\n",
    "test_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e259986d-841a-494a-9d70-01f8c3a51312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion: Focused (Confidence: 1.0000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# ================================\n",
    "# 1. 설정\n",
    "# ================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 3\n",
    "weights_path = \"/Users/kimjohyeon/Desktop/Zypt_Model/checkpoint_epoch_1_val_metric_0.9538.pt\"  # 저장한 가중치 파일 경로\n",
    "\n",
    "# ================================\n",
    "# 2. 모델 로드 및 가중치 적용\n",
    "# ================================\n",
    "model = timm.create_model(\"mobilenetv3_large_100\", pretrained=False, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ================================\n",
    "# 3. 이미지 전처리 정의\n",
    "# ================================\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # MobileNetV3 입력 크기\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean/std\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ================================\n",
    "# 4. 감정 라벨\n",
    "# ================================\n",
    "emotion_labels = {0: \"Focused\", 1: \"Not Focused\", 2: \"Drowsy\"}\n",
    "\n",
    "# ================================\n",
    "# 5. 이미지 평가 함수\n",
    "# ================================\n",
    "def predict(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = preprocess(image).unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)  # logits\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        pred_class = torch.argmax(probs, dim=1).item()\n",
    "        pred_label = emotion_labels[pred_class]\n",
    "\n",
    "    print(f\"Predicted emotion: {pred_label} (Confidence: {probs[0][pred_class]:.4f})\")\n",
    "\n",
    "# ================================\n",
    "# 6. 테스트 실행\n",
    "# ================================\n",
    "test_image_path = \"/Users/kimjohyeon/Desktop/Capstone/Sample8.jpg\"\n",
    "predict(test_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4ecb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
